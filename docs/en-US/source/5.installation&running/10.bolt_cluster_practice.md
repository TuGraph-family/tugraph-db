# Bolt High Availability Cluster Mode Practice

> This document primarily explains how to connect to and use a high availability cluster via the Bolt protocol.

## Create Cluster

The following is an example of creating a cluster with three nodes.

### Preparation Steps

Pull the image on each node and start the Docker container.

1. Pull the Image

```
docker pull tugraph/tugraph-runtime-centos7:latest
```

2. Start the Docker Container.

```
docker run --net=host -itd -v /root/tugraph/data:/var/lib/lgraph/data  -v /root/tugraph/log:/var/log/lgraph_log \
--name tugraph_ha ${REPOSITORY}:${VERSION} /bin/bash
```

### Node Configuration

Modify the configuration file on each node. By default, the configuration file is located at /usr/local/etc/lgraph.json inside the container.

Node 1

```json
{
  "directory" : "/var/lib/lgraph/data",
  "host" : "172.20.10.52",
  "port" : 7070,
  "bolt_port": 7687,
  "bolt_raft_port":8000,
  "bolt_raft_node_id":1,
  "bolt_raft_init_peers":[
    {"bolt_raft_node_id":1,"ip":"172.20.10.52","bolt_raft_port":8000,"bolt_port":7687},
    {"bolt_raft_node_id":2,"ip":"172.20.10.53","bolt_raft_port":8000,"bolt_port":7687},
    {"bolt_raft_node_id":3,"ip":"172.20.10.54","bolt_raft_port":8000,"bolt_port":7687}
  ],
  "verbose" : 1,
  "log_dir" : "/var/log/lgraph_log",
  "disable_auth" : false,
  "ssl_auth" : false,
  "server_key" : "/usr/local/etc/lgraph/server-key.pem",
  "server_cert" : "/usr/local/etc/lgraph/server-cert.pem",
  "web" : "/usr/local/share/lgraph/browser-resource"
}
```

Node 2

```json
{
  "directory" : "/var/lib/lgraph/data",
  "host" : "172.20.10.53",
  "port" : 7070,
  "bolt_port": 7687,
  "bolt_raft_port":8000,
  "bolt_raft_node_id":2,
  "bolt_raft_init_peers":[
    {"bolt_raft_node_id":1,"ip":"172.20.10.52","bolt_raft_port":8000,"bolt_port":7687},
    {"bolt_raft_node_id":2,"ip":"172.20.10.53","bolt_raft_port":8000,"bolt_port":7687},
    {"bolt_raft_node_id":3,"ip":"172.20.10.54","bolt_raft_port":8000,"bolt_port":7687}
  ],
  "verbose" : 1,
  "log_dir" : "/var/log/lgraph_log",
  "disable_auth" : false,
  "ssl_auth" : false,
  "server_key" : "/usr/local/etc/lgraph/server-key.pem",
  "server_cert" : "/usr/local/etc/lgraph/server-cert.pem",
  "web" : "/usr/local/share/lgraph/browser-resource"
}
```

Node 3

```json
{
  "directory" : "/var/lib/lgraph/data",
  "host" : "172.20.10.54",
  "port" : 7070,
  "bolt_port": 7687,
  "bolt_raft_port":8000,
  "bolt_raft_node_id":3,
  "bolt_raft_init_peers":[
    {"bolt_raft_node_id":1,"ip":"172.20.10.52","bolt_raft_port":8000,"bolt_port":7687},
    {"bolt_raft_node_id":2,"ip":"172.20.10.53","bolt_raft_port":8000,"bolt_port":7687},
    {"bolt_raft_node_id":3,"ip":"172.20.10.54","bolt_raft_port":8000,"bolt_port":7687}
  ],
  "verbose" : 1,
  "log_dir" : "/var/log/lgraph_log",
  "disable_auth" : false,
  "ssl_auth" : false,
  "server_key" : "/usr/local/etc/lgraph/server-key.pem",
  "server_cert" : "/usr/local/etc/lgraph/server-cert.pem",
  "web" : "/usr/local/share/lgraph/browser-resource"
}
```

Configuration Instructions:

* `port`: web portã€‚
* `bolt_port`: External port for Bolt client connections.
* `bolt_raft_port`: For internal cluster use, this is the port used for communication between nodes in the Raft cluster. Bolt clients should not connect to this port.
* `bolt_raft_node_id`: Raft cluster node ID, which must be unique for each node in the entire cluster.
* `bolt_raft_init_peers`: Used during the initial setup of the Raft cluster, it contains each node's bolt_raft_node_id, ip, bolt_raft_port, and bolt_port. This allows every node to know the communication details of the other nodes, enabling them to communicate with each other.

The four core configuration parameters for a Raft cluster are bolt_port, bolt_raft_port, bolt_raft_node_id, and bolt_raft_init_peers.

Please note that configuration options used when creating a cluster via the RPC protocol, such as enable_ha and ha_node_join_group_s, should not appear in the configuration file used to create a High Availability (HA) cluster via Bolt.

### Start the Service

To start the service in the tugraph_ha container, you need to follow these steps:
1.Enter the Container:

```
docker exec -it tugrapn_ha bash
```

2.Start the Service:

```
lgraph_server -c /usr/local/etc/lgraph.json -d start
```

### View Cluster Member Information

You can send a request to any node in the cluster to return the member list and leader information.

``
CALL db.bolt.listRaftNodes()
``

### View Node Raft Status Information

You can send a request to any node in the cluster to return its own raft status information.

``
CALL db.bolt.getRaftStatus()
``

## Remove/Add Nodes

Below is a simulation of an unexpected service stop for a node (by killing that node's service process), removing the stopped node, and adding a new node.

### Remove Node

Remove the instance with node ID 3 from the cluster. This can only be executed on the leader node.

```
CALL db.bolt.removeRaftNode(3)
```

After removal, you can check the cluster member information again to see that the instance with ID 3 has been cleared.

### Add New Node

Add an instance with node ID 4. For initialization, refer to the "Preparation" section on creating a cluster. At this point, don't start the lgraph_server service yet.

Add a new node to the cluster with node ID 4, IP 172.20.10.41, bolt_port 7687, bolt_raft_port 8000. Adding a new node can only be executed on the leader node.

```
 CALL db.bolt.addRaftNode(4, '172.20.10.41', 7687, 8000)
```

It's important to note that after executing addRaftNode to add a new node, you need to manually perform an offline full data synchronization before starting the lgraph_server service on the new node. Once this process is complete and the new node service is started and running, any further incremental data writes will be automatically synchronized. Full data synchronization can be done by replacing the tugraph directory mounted during the execution of the docker run command. The specific steps are as follows:
1. Stop the new node's container.
2. Replace the tugraph directory from any working service node with the tugraph directory mounted during the docker run command for the new node.
3. Start the new node's container.
4. Enter the container and modify the configuration file. Here is an example configuration:

```
 {
"directory" : "/var/lib/lgraph/data",
"host" : "172.20.10.41",
"port" : 7070,
"bolt_port": 7687,
"bolt_raft_port":8000,
"bolt_raft_node_id":4,
"bolt_raft_init_peers":[
{"bolt_raft_node_id":1,"ip":"172.20.10.52","bolt_raft_port":8000,"bolt_port":7687},
{"bolt_raft_node_id":2,"ip":"172.20.10.53","bolt_raft_port":8000,"bolt_port":7687},
{"bolt_raft_node_id":4,"ip":"172.20.10.41","bolt_raft_port":8000,"bolt_port":7687}
],
"verbose" : 1,
"log_dir" : "/var/log/lgraph_log",
"disable_auth" : false,
"ssl_auth" : false,
"server_key" : "/usr/local/etc/lgraph/server-key.pem",
"server_cert" : "/usr/local/etc/lgraph/server-cert.pem",
"web" : "/usr/local/share/lgraph/browser-resource"
}
```

5.Start the service:

```
lgraph_server -c /usr/local/etc/lgraph.json -d start
```

## Add Learner Node

```
CALL db.bolt.addRaftLearnerNode(5, '172.20.10.42', 7687, 8000)
```

This can only be executed on the leader node. The specific process can refer to the above "Add Node" steps. The learner node does not participate in elections or voting, but only synchronizes data.

## Bolt Client Connection Example
### Python
```
from neo4j import GraphDatabase

def custom_resolver(address):
    if address == ('mycluster.com', 9999):
        return [
            ('172.20.10.52', 7687),
            ('172.20.10.53', 7687),
            ('172.20.10.54', 7687)
        ]
    else:
        return [address]


if __name__ == '__main__':
    driver = GraphDatabase.driver(
        "neo4j://mycluster.com:9999",
        auth=("admin", "73@TuGraph"),
        resolver=custom_resolver
    )
    session = driver.session(database="default")
    session.run("CALL db.dropDB()")
    session.run("CALL db.createVertexLabel('person', 'id' , 'id', 'INT32', false, 'count', 'INT64', false)")
    session.run("create (n1:person {id:1, count:0})")
    session.close()
    driver.close()
```

mycluster.com:9999 is a fictitious domain address. The third argument in the GraphDatabase driver initialization is to pass a custom resolver custom_resolver. The logic in custom_resolver is to return three cluster addresses when the domain address is mycluster.com:9999.